{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/chatbots-talking.jpg width=500>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Conversations on Twitter ###\n",
    "\n",
    "We've spent the last few days refining tools to help us get at how conversations play out on Twitter. With Twitter's talk of efforts to promote civility and organic discussion, we are inevitably led to try to understand the flow of information between accounts on Twitter, say - and how this flow interacts with web sites, news articles, and YouTube videos. We finished our session last time with a ovely tool, Graph Commons, that let you visualize the patterns of infomation flow. Did anyone play with it at all? It's realy useful. \n",
    "\n",
    "Graphing networks is an extremely powerful reporting and storytelling tool. Aside from Graph Commons, consider Little Sis (as opposed to Big Brother). It is more hands-on. \n",
    "\n",
    "<img src=\"https://github.com/computationaljournalism/columbia2020/raw/master/images/ls1.jpeg\" width=500 style=\"border: #000000 1px outset;\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://github.com/computationaljournalism/columbia2020/raw/master/images/ls1.jpeg\" width=500 style=\"border: #000000 1px outset;\">\n",
    "\n",
    "Little Sis takes its inspiration not from journalism per se, but from an artist Mark Lombardi. [Ben Fry from Fathom has written](https://benfry.com/exd09/) about Lombardi's lovely hand-drawn networks (with titles like “George W. Bush, Harken Energy, and Jackson Stephens. 1979-90” and “World Finance Corporation, Miami, ca. 1970-84.” \n",
    "\n",
    "<img src=\"https://benfry.com/exd09/full/04.png\" width=500>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Cascade from the NYT**\n",
    "\n",
    "In an attempt to understand the conversation around its articles, the New York Times performed an experiment in the \"golden age\" of Twitter data access (remember Gnip?). The Times merged data from three sources -- its web access logs. (We saw those from `digg.com` when Mike showed us how accesses to their web site was recorded, item by item, line by line.) They also had logs from `bit.ly` which is the URL shortener behind `nyti.ms`, watching people shorten URLs. Finally, with Twitter data, they could see references to their articles float across the network -- all the mentions, favorites, retweets and replies.  \n",
    "<br><br>\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/data%20docs%201.001.jpeg width=500>\n",
    "<br><br>\n",
    "**Lesson \\#1 of data science — data are promiscuous!** These three data sets can be \"linked.\" The web access logs owned by the Times include your IP address as part of the browsing transaction. Ah and `bit.ly` also has your IP address when you use their API to shorten a URL. [(Um, yes, `bit.ly` has an API!](https://dev.bitly.com/v4_documentation.html). So browsing and shortening can be linked. Then, we can watch Twitter for the appearance of your shortened link. This is a timing thing and connections are looser, but still \"good enough for government work\" as we used to say at NASA — I mean in a joking way.\n",
    "\n",
    "We can then watch `bit.ly` for when people click their links, expand a URL and land at an article on `nytimes.com`. Associating the particular tweet with a `bit.ly` link used to be a piece of math. ([Which we even patented!](http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&d=PALL&p=1&u=%2Fnetahtml%2FPTO%2Fsrchnum.htm&r=1&f=G&l=50&s1=9,135,576.PN.&OS=PN/9,135,576&RS=PN/9,135,576) Remember that blissful time when software patents weren't pure evil? Oh and these days CMS's generate unique URLs making this process less math-fussy.) And, for a final flourish, we can watch the person who clicked on your (presumably your) tweet and see what kind of browsing they do on the Times' site.\n",
    "\n",
    "<br><br>\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/data%20docs%201.002.jpeg width=500>\n",
    "<br><br>\n",
    "OK so that's the whole transaction. And so we build. For each article appearing in the Times, we could track their activity on Twitter. What networks get activated? Who retweets whom? We found beautiful quiet networks... like the group of rabbis that shared each religion story and commented amongst themselves. The simple tools of who shares what with whom, who is talking to whom, but seen over time — these provide us with great investigative angles for stories. To help, we wrapped it up with a lovely graphical interface. \n",
    "\n",
    "**Lesson \\#2 of data science — data need a face!** There is no natural look to data and when we have a huge system like Twitter, we need help bringing them down to an understandable scale. [Lev Manovich calls this the anti-sublime.](http://manovich.net/index.php/projects/data-visualisation-as-new-abstraction-and-anti-sublime) Anyway, that's enough of the digital humanities for one day. Here's what Jer Thorp and I came up with to provide Cascade with a humanly accessible \"face.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "VimeoVideo(54858819,width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see what we can do**\n",
    "\n",
    "There are services that help narrow generic tools like Graph Commons for use on Twitter data. First, we are going to have a look at conversations that mention `\"Fidel Castro\"`. Our first look will be through a platform called [Hoaxy](https://hoaxy.iuni.iu.edu). It's a platform not a programming approach which means you might outgrow it quickly, but you should have an idea about how to create similar graphics from scratch. Let's have a look at a [\"network diagram\"](https://hoaxy.iuni.iu.edu/#query=Fidel%20Castro&sort=mixed&type=Twitter&lang=) that maps out who is mentioning, retweeting, replying to whom.\n",
    "\n",
    "Hoaxy produces a network diagram, where each point represents an account on Twitter. The connections represent two accounts \"in communication\" by retweeting, replying or mentioning one another (with the relationship being described as *directional* -- I might mention you, but you might not ever mention me). The colors of the points depict a scale that is meant to describe the \"bot like\"ness of an account. \n",
    "<br><br>\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/b1.jpg width=500 style=\"border:1px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here is the network. And there is so much to dig into... so many different pieces of \"information\" floating around and entering in and out of the conversation. Play with it a little. Explore, read, change.\n",
    "\n",
    "**Bot, bot, bot, goose!**\n",
    "\n",
    "Where do the colors of the nodes come from? The good people at Indiana Univeristy have \"learned\" characteristics of bot behavior, producing a score for any account. The learning (which we will return to formally later in the term) involves contrasting data from known bot accounts and \"real\" accounts. Each Twitter account is reduced to a number of characteristics to make this comparison. The list below was taken from one of their academic papers. \n",
    "<br><br>\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/b8.jpg width=400 style=\"border:1px solid black\">\n",
    "<br><br>\n",
    "In general, this is a hard learning problem. But the color scale might give you a rough indication of when the conversation taking place involves authentic or somehow immitation accounts. And we might want to know the difference because it will help us judge whether we are witnessing (or partaking in) a genuine exchange of opinions or are instead part of an amplification campaign of some kind. Propaganda.\n",
    "\n",
    "**Lesson \\#3 of data science — Never trust an algorithm you didn't write, and even then be very cautious!** We can use these scores in various ways, always checking to see if they make sense, of course. Here we look at Pete Buttigieg's followers...\n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/tg1.jpg width=500 style=\"border:1px solid black\">\n",
    "\n",
    "...  and the people he follows.\n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/tg2.jpg width=500 style=\"border:1px solid black\">\n",
    "\n",
    "**For you — Ten minutes**\n",
    "\n",
    "From the BotoMeter interface, request a group of your followers and export the data (there is a button at the bottom of the page). Download it and bring it into your notebook. Then, compute the median content score of this collection of your followers. Ready? Go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might also consider exploring this recommendation from last evening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">My odd interest in digital tooling for political campaigns brought me to <a href=\"https://twitter.com/PeteButtigieg?ref_src=twsrc%5Etfw\">@PeteButtigieg</a> Twitter during last night’s debate. I’m convinced they run 1000’s of fake accounts that all follow and retweet each other. Would love to see a <a href=\"https://twitter.com/cocteau?ref_src=twsrc%5Etfw\">@cocteau</a> “Follower Factory”-style analysis done. <a href=\"https://t.co/iPHvxAOJHy\">pic.twitter.com/iPHvxAOJHy</a></p>&mdash; Rune Madsen (@runemadsen) <a href=\"https://twitter.com/runemadsen/status/1232685573304987648?ref_src=twsrc%5Etfw\">February 26, 2020</a></blockquote> \n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The group at Indiana has made their bot-or-not scaling available via an API complete with a [Python interface](https://github.com/IUNetSci/botometer-python)! You can install the `botometer` and then use your Twitter credentials and a key from RapidAPI. You will need a key to use the API -- you are limited to 2,000 calls per day. You can apply [here](https://rapidapi.com/OSoMe/api/botometer?utm_source=mashape&utm_medium=301) (RapidAPI is an API hosting service of sorts.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install botometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab your keys from a previous notebook or https://apps.twitter.com\n",
    "\n",
    "CONSUMER_KEY = \"\"\n",
    "CONSUMER_SECRET = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "ACCESS_TOKEN_SECRET = \"\"\n",
    "\n",
    "\n",
    "# here's my key - i'm not sure it will work for you or if it's tied to my twitter account\n",
    "MashKey = \"NAeSF7TTbymshxelRHhCXqlQVfc0p1zgYQojsnVombDJddPvas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's kick the tires on this! We can look up one person or a group. Let's try a few..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botometer import Botometer\n",
    "\n",
    "meter = Botometer(wait_on_ratelimit=True,\n",
    "                  mashape_key=MashKey,\n",
    "                  consumer_key = CONSUMER_KEY,\n",
    "                  consumer_secret = CONSUMER_SECRET,\n",
    "                  access_token = ACCESS_TOKEN,\n",
    "                  access_token_secret = ACCESS_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a single account by screen name\n",
    "result = meter.check_account('@iamsandali')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = meter.check_account('@abbieLJ')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All about (or a lot about... or some about) bots! ###\n",
    "\n",
    "Today we wanted to give you a little overview of the kinds of bots that are circulating in the world, providing a taxonomy, if you will, of all the innovative work being done. But first, what is a bot? \n",
    "\n",
    "We'll draw mostly from [botnerds.com](http://botnerds.com) and the [article by Mark Sample](https://medium.com/@samplereality/a-protest-bot-is-a-bot-so-specific-you-cant-mistake-it-for-bullshit-90fe10b7fbaa#.ikymcdf6q). The former reference offers a practical definition...\n",
    ">Bots are software programs that perform automated, repetitive, pre-defined tasks.  These tasks can include almost any interaction with software that has an API.\n",
    "\n",
    "...while Mark Sample is more aspirational.\n",
    ">A computer program that reveals the injustice and inequality of the world and imagines alternatives. A computer program that says who’s to praise and who’s to blame. A computer program that questions how, when, who and why. A computer program whose indictments are so specific you can’t mistake them for bullshit. A computer program that does all this automatically.\n",
    "\n",
    "And then of course, the State of California has defined a bot in it's recently passed bot legislation.\n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2019/raw/master/images/u5.001.jpeg>\n",
    "\n",
    "So let's dig in a bit and send you off with a bot homework!\n",
    "\n",
    "![Bot](http://21clradio.com/wp-content/uploads/2016/02/Kiddle-Logo.png)\n",
    "\n",
    "First, the practical. Our first reference divides bots into \"Good\" and \"Bad\" categories, which are interpreted largely in terms of their effect on our information ecosystem -- functionally, adding or detracting.\n",
    "\n",
    "1. Good Bots\n",
    "    * **Chatbots** \"are designed to carry on conversations with humans, usually just for fun, and to test the limits of the technology.\"\n",
    "    * **Crawlers** \"run continuously in the background, primarily fetch data from other APIs or websites, and are *well-behaved* in that they respect directives you give them\"\n",
    "    * **Transactional bots** act as agents on behalf of humans, and interact with external systems to accomplish a specific transaction, moving data from one platform to another.\"\n",
    "    * **Informational bots** \"surface helpful information, often as push notifications, and are also called *news bots*.\"\n",
    "    * **Art bots** \"are designed to be appreciated aesthetically.\"\n",
    "    * **Game bots** \"game bots function as characters, often for humans to play against or to practice and develop skills...\"\n",
    "2. Bad Bots\n",
    "    * **Hackers** \"distribute malware of all kinds\"\n",
    "    * **Spammers** \"steal content (email addresses, images, text, etc) from other website\", often to republish it\n",
    "    * **Scrapers** post \"promotional content around the web, and ultimately drive traffic to the spammer’s website\"\n",
    "    * **Impersonators** \"mimic natural user characteristics, making them hard to identify\" (they cite [political propadanda bots](http://www.businessinsider.com/political-bots-by-governments-around-the-world-2015-12/#mexico-1))\n",
    "    \n",
    "In terms of Bot Agency or Bot Intelligence, this framing presents examples along a spectrum -- Script Bots, Smart Bots and Intelligent Agents. An interaction with a Script Bots, they write, is\n",
    "\n",
    ">based off of a pre-determined model (the “script”) that determines what the bot can and cannot do.  The “script” is a decision tree where responding to one question takes you down a specific path, which opens up a new, pre-determined set of possibilities. \n",
    "\n",
    "Upping the autonomy a little, Smart Bots have access to other APIs that expand the universe of responses.\n",
    "\n",
    ">Many bots have a heavy server-side processing component, which allows them access to massive computing power in understanding and responding to queries.  Couple that with the open-sourcing of AI software libraries like Theano and TensorFlow, and you have the ingredients for some amazing human-bot interactions.\n",
    "\n",
    "This category also allows for human-assisted interactions. The bot need not act alone, but can invoke human intelligence or even direct consultation, redirecting the interaction to a responsible human. Finally, the Intelligent Agent is meant to act autonomously.\n",
    "\n",
    "> If operating correctly, they should require no human intervention in order to perform their tasks correctly.  Google’s self-driving cars are designed without steering wheels for humans, because they shouldn’t be necessary.  x.ai has a bot that schedules meetings for you, Amy Ingram, and she manages all the back-and-forth with zero oversight.\n",
    "\n",
    "Mark Sample provides a different, less practical characterization of bots, one drawn more from literary studies (where bots have been an object of fascination for some time). He focuses on one particular kind of bot (primarily active on Twitter) that he terms \"Protest Bots\" or \"Bots of Conviction\". Sample says they share at least five characteristics: \n",
    "\n",
    "* **Topical** - \"They are about the morning news — and the daily horrors that fail to make it into the news.\"\n",
    "* **Data-based** - \"They don’t make this [stuff] up. They draw from research, statistics, spreadsheets, databases.\" \n",
    "* **Cumulative** - It is the nature of bots to do the same thing over and over again, with only slight variation...  The repetition builds on itself, the bot relentlessly riffing on its theme, unyielding and overwhelming, a pile-up of wreckage on our screens.\"\n",
    "* **Oppositional** - \"Bots of conviction challenge us to consider our own complicity in the wrongs of the world.\"\n",
    "* **Uncanny.** - \"Protests bots often reveal something that was hidden; or conversely, they might purposefully obscure something that had been in plain sight.\"\n",
    "\n",
    "The examples of Protest Bots that Sample introduces are often journalistic, but often more in the realm of advocacy. Still, the examples open up the potential to the kinds of projects or actions that can be taken outside the more functional description of our first reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">The next person to be executed in Ohio is Cleveland Jackson in 3 months, 8 days, 22 hours and 13 minutes. <a href=\"https://t.co/uD8HuiZIfM\">https://t.co/uD8HuiZIfM</a></p>&mdash; The Next To Die (@thenexttodie) <a href=\"https://twitter.com/thenexttodie/status/1098247685386264576?ref_src=twsrc%5Etfw\">February 20, 2019</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This weekend, you will be imaginging and making your own bots and we hope this outline has helped prime the pump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Bot Account & App\n",
    "\n",
    "**1) Create a New Twitter User for Your Bot**\n",
    "\n",
    "Before we get started, you'll want to create a new Twitter account for your bot! It's best to not create a bot out of your personal Twitter account, but I will leave that up to you!\n",
    "\n",
    "Go to [https://twitter.com/signup](https://twitter.com/signup) (you'll have to log out of your normal account or go incognito) and create a new account. **You will need to use your phone number when siging up** or you wont be able to create a new Twitter app in next step. You can sign up for a [https://voice.google.com/](Google Voice number) if you don't want to use your own phone number, or if Twitter gives you a hard time for having too many accounts tied to a single phone number.\n",
    "\n",
    "\n",
    "**2) Create a New Twitter App for Your Bot**\n",
    "\n",
    "You are a pro at this! Once you have created your new Twitter account, create a new Twitter app for your bot.\n",
    "\n",
    "1. Go to [https://apps.twitter.com](https://apps.twitter.com/) and log in with your new Twitter user account.\n",
    "2. Click “Create New App”\n",
    "3. Fill out the form, agree to the terms, and click “Create your Twitter application”\n",
    "4. Click on “Keys and Access Tokens” tab, and copy your “API key” and “API secret”. Scroll down and click “Create my access token”, and copy your “Access token” and “Access token secret”.\n",
    "\n",
    "Once you have your tokens, copy them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your own keys and secrets here...\n",
    "CONSUMER_KEY = \"\"\n",
    "CONSUMER_SECRET = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "ACCESS_TOKEN_SECRET = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we can make Twitter API calls, we need to initialize a few things...\n",
    "from tweepy import OAuthHandler, API\n",
    "\n",
    "# setup the authentication\n",
    "auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "# create an object we will use to communicate with the Twitter API\n",
    "api = API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And maybe print a little \"It's alive\" message..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the \"me\" api to make sure you using the Twitter api as your bot\n",
    "print('Ok, we are ready to tweet as ' + api.me().screen_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Send a Tweet From Our Bot!\n",
    "\n",
    "We will be using the `statuses/update` api to send the tweet: https://dev.twitter.com/rest/reference/post/statuses/update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your bot's first tweet\n",
    "api.update_status(status='Whe the people...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Great! Now let's take a quick detour....\n",
    "\n",
    "In a few minutes, we are going to create a simple \"news\" bot: a bot that tweets out the latest stories from The New York Times. But, how are we going to get the NYTimes latest stories? Let's turn to [RSS](https://en.wikipedia.org/wiki/RSS).\n",
    "\n",
    "Most news sites on the internet publish an RSS feed. Here are a few:\n",
    "\n",
    "[New York Times \"HomePage\"](http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml)\n",
    "\n",
    "[Wired](https://www.wired.com/feed/)\n",
    "\n",
    "[WNYC Radio Lab Podcast](http://feeds.wnyc.org/radiolab)\n",
    "\n",
    "To use RSS feeds in our code, we're going to use the python module called [Feedparser](https://pypi.python.org/pypi/feedparser). `Feedparser` does the hard work of fetching and parsing the feeds for us. RSS feeds can be very messy and this module does an amazing job of dealing with the mess and handing us a nice python object (`dictionary`) to work with! \n",
    "\n",
    "Let's install the module and start working with some RSS feeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following to install the `feedparser` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install feedparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a short example of how we can fetch the RSS feed for The New York Times \"HomePage\" stories.\n",
    "\n",
    "The \"HomePage\" RSS feed can be found here: [http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml](http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml)\n",
    "\n",
    "The code below uses the `feedparser` module to fetch the RSS feed (remember HTTP requests?), parse it and return it as a python dictionary. This module does the hard work for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's fetch the New York Times Homepage RSS Feed\n",
    "from feedparser import parse\n",
    "\n",
    "# the URL of the homepage stories RSS feed\n",
    "nytimes_rss_url = 'http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml'\n",
    "\n",
    "# fetch the RSS feed and parse it\n",
    "feed = parse(nytimes_rss_url)\n",
    "\n",
    "# what type of object are we dealing with?\n",
    "print(type(feed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, the function `parse` will return a dictionary-like object, meaning we store our data under `keys()`. Let's see what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feed.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then let's have a closer look at the `feed` information..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed['feed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of thing do we have? What kind of data do we have? Now, lets look at the `entities`, which is a list of the stories found in the feed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's print out the stories (titles and urls) in the RSS feed\n",
    "for entry in feed['entries']:\n",
    "    print(entry['title'])\n",
    "    print(entry['link'])\n",
    "    print('--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a break from \"news\" and look at the RSS feed for [Atlas Obscura](http://www.atlasobscura.com/) (a lovely site about travel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedparser import parse\n",
    "\n",
    "rss_url = 'http://www.atlasobscura.com/feeds/latest'\n",
    "feed = parse(rss_url)\n",
    "\n",
    "for entry in feed['entries']:\n",
    "    print(entry['title'])\n",
    "    print(entry['link'])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OK, Let's Get Back to Our Bot!\n",
    "\n",
    "If we wanted to tweet out the latest story from Atlas Obscura, we combine our Twitter and our RSS/feedparser examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler, API\n",
    "from feedparser import parse\n",
    "\n",
    "# setup the authentication\n",
    "auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "# create an object we will use to communicate with the Twitter API\n",
    "api = API(auth)\n",
    "\n",
    "# now, get the Atlas Obscura feed\n",
    "rss_url = 'http://www.atlasobscura.com/feeds/latest'\n",
    "feed = parse(rss_url)\n",
    "\n",
    "# let's take only the 1st story in our list\n",
    "first_story = feed['entries'][0]\n",
    "\n",
    "# now, create the text of the tweet using the story title and link/url\n",
    "tweet_text = 'This is really interesting! ' + first_story['title'] + ' ' + first_story['link']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at what we're about to tweet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, tweet it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.update_status(status=tweet_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Up: Let's Clone the @twoheadlines Bot\n",
    "\n",
    "[Darius Kazemi](https://twitter.com/tinysubversions) created a clever bot called [@twoheadlines](https://twitter.com/twoheadlines) where he combines two different headlines in to a single tweet:\n",
    "\n",
    "> Comedy is when you take two headlines about different things and then confuse them\n",
    "\n",
    "Let's do a simple clone of the `@twoheadlines` bot by combining the first half of a New York Times headline with the second half of a Breitbart headline :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feedparser import parse\n",
    "\n",
    "# fetch the nytimes and breitbart RSS feeds\n",
    "nytimes_rss_url = 'http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml'\n",
    "breitbart_rss_url = 'http://feeds.feedburner.com/breitbart'\n",
    "\n",
    "nytimes_feed = parse(nytimes_rss_url)\n",
    "breitbart_feed = parse(breitbart_rss_url)\n",
    "\n",
    "# get the first story from each of the two feeds\n",
    "nytimes_first_story = nytimes_feed['entries'][0]\n",
    "breitbart_first_story = breitbart_feed['entries'][0]\n",
    "\n",
    "print('nyt: ',nytimes_first_story['title'],\"\\n\")\n",
    "print('breit: ',breitbart_first_story['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two headlines into a single headline\n",
    "nytimes_words = nytimes_first_story['title'].split(' ')\n",
    "breitbart_words = breitbart_first_story['title'].split(' ')\n",
    "\n",
    "# take the 1st half of the nytimes \"words\" plus the second half of the breitbart \"words\n",
    "new_words = nytimes_words[:len(nytimes_words)//2] +\\\n",
    "            breitbart_words[len(breitbart_words)//2:]\n",
    "\n",
    "# this is python weirdness to take a list of words\n",
    "# and join them together with a space between each word\n",
    "new_headline = ' '.join(new_words)\n",
    "print(new_headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your bot can tweet the combined headline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.update_status(status=new_headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK, that's cute but how can we create a long-running bot?\n",
    "\n",
    "Everything we've done up to now just runs once and then exits/stops. Let's look at how we can have something run forever - our bot doesn't need to sleep much!\n",
    "\n",
    "Python has a great [`time`](https://docs.python.org/2/library/time.html), which handles various time-related functions (duh!). The `time` module also has a very helpful method called `sleep()`, which tells our program to sleep, or \"pause\", for a number of seconds. Let's take a look at it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the time module allows us to \"sleep\" or pause for a given number of seconds\n",
    "from time import sleep\n",
    "\n",
    "# loop 10 times, pausing for 1 second during each iteration\n",
    "for number in range(0, 10):\n",
    "    print(number)\n",
    "    \n",
    "    # sleep for one second\n",
    "    sleep(1)\n",
    "    \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a simple \"forever\" loop to get our script to run until we stop it. The code below will loop forever, pausing for 1 second, until you hit the stop button in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the time module allows us to \"sleep\" or pause for a given number of seconds\n",
    "from time import sleep\n",
    "\n",
    "# loop forever!\n",
    "while True:\n",
    "    print('hello')\n",
    "    \n",
    "    # sleep for one second\n",
    "    time.sleep(1)\n",
    "    \n",
    "# to get this to stop, hit the Stop button in your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Let's put it all together and build our news bot\n",
    "\n",
    "This is a very simple \"news\" bot, which will tweet out new top stories from The New York Times. The bot will check the NYTimes HomePage RSS feed every 10 seconds - if it sees a new story, it will tweet it.\n",
    "\n",
    "I'm also adding some super complicated AI, to add some color-commentary to each story that our bot tweets.\n",
    "\n",
    "This code uses a new module called [`random`](https://docs.python.org/2/library/random.html), which makes it easy to randomly select an item from a `list`.\n",
    "\n",
    "*So you don't put extra stress on The New York Times servers, you should sleep every 60 seconds (at least). We are only sleeping for 10 seconds here for demo purposes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this \"bot\" will tweet out any new stories published in the nytimes homepage\n",
    "from time import sleep    \n",
    "from feedparser import parse\n",
    "from random import choice\n",
    "\n",
    "insightful_things_to_say = [\n",
    "    'this is really interesting',\n",
    "    'great read -->',\n",
    "    'hmmm....',\n",
    "    'amazing',\n",
    "    'how does this happen?',\n",
    "]\n",
    "\n",
    "nytimes_rss_url = 'http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml'\n",
    "\n",
    "# keep track of the previous nytimes link/url that we tweeted\n",
    "prev_tweeted_link = ''\n",
    "\n",
    "# loop forever!\n",
    "while True:\n",
    "    \n",
    "    # fetch and parse the NYTimes RSS feed\n",
    "    nytimes_feed = parse(nytimes_rss_url)\n",
    "\n",
    "    # get the first story\n",
    "    first_story = nytimes_feed['entries'][0]\n",
    "\n",
    "    # take the link of the first story and see if we've tweeted it before\n",
    "    link = first_story['link']\n",
    "    if link != prev_tweeted_link:\n",
    "        # it's new, lets tweet it out!\n",
    "        print('new story - lets tweet it: ' + link)\n",
    "  \n",
    "        # build the text of our tweet\n",
    "        tweet_text = choice(insightful_things_to_say) + ' ' + first_story['title'] + ' ' + first_story['link']\n",
    "        \n",
    "        # fire it off to twitter\n",
    "        api.update_status(status=tweet_text)\n",
    "        \n",
    "        # keep track of the this link that we just tweeted\n",
    "        prev_tweeted_link = link\n",
    "    else:\n",
    "        # we've already tweeted this...no new stories\n",
    "        # nothing to do\n",
    "        print('no new story...lets wait a little while')\n",
    "\n",
    "    # sleep for a little while\n",
    "    sleep(10)\n",
    "\n",
    "    \n",
    "# if you want to stop this script, hit the Stop button in your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For homework, you will create a bot that responds to data in realtime, perhaps retweeting another account..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">I&#39;m a bot that retweets <a href=\"https://twitter.com/realDonaldTrump?ref_src=twsrc%5Etfw\">@realDonaldTrump</a> so you don&#39;t have to follow him. <a href=\"https://twitter.com/hashtag/unfollowtrump?src=hash&amp;ref_src=twsrc%5Etfw\">#unfollowtrump</a></p>&mdash; I Retweet Trump (@IRetweetTrump) <a href=\"https://twitter.com/IRetweetTrump/status/782440751372251136?ref_src=twsrc%5Etfw\">October 2, 2016</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or perhaps look at what goes missing. [Politwoops](https://projects.propublica.org/politwoops/), now supported by ProPublica is a good example. \n",
    "\n",
    "## Some computing tools for programming with \"language\"\n",
    "\n",
    "The Twitter bot Mike prepared relies on mashing up two headlines. Some of that might get better if we knew a little about what the headline described. What is the subject? What action is described? Some of these questions are addressed by a field of computer science (well, computational linguistics) called Natural Language Processing. There are plenty of tools in Python for making use of the fruits of this research. \n",
    "\n",
    "We will be using a package called [TextBlob](https://textblob.readthedocs.io/en/dev/) that is a simplified version of the Natural Language Toolkit in Python. (Sometimes tools become really powerful for practitioners and leave non-experts behind. That's what has happened, to some extent, with the NLTK. It's a little hard to just \"jump in\". And so TextBlob is like computational training wheels.) [Allison Parrish's Natural Language Basics with TextBlob](http://rwet.decontextualize.com/book/textblob/) is a great place to read about what TextBlob is good for. \n",
    "\n",
    "First, we need to install the package. Off to PIP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "download('brown')\n",
    "download('punkt')\n",
    "download('maxent_ne_chunker')\n",
    "download('words')\n",
    "download('conll2000')\n",
    "download('maxent_treebank_pos_tagger')\n",
    "download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, load the package for this session and bring in a lede from todays New York Times. We read it in as a string but preface the quotes with a \"u\". That tells Python the string is in Unicode -- publishers use fancy quotation marks, for example, that are not the simple \" or '. \n",
    "\n",
    "The `TextBlob()` function takes text and turns it into a `\"TextBlob\"` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "headline = u\"New cases of the coronavirus popping up across Europe. Dozens of infections in Iran stoking fears about an uncontrolled spread in the Middle East. Global market jitters continuing after a steep slide. American health authorities warning that it was a matter of when, not if, the epidemic would reach the United States. A toxic political climate in Washington complicating the public health challenge.\"\n",
    "tb = TextBlob(headline)\n",
    "\n",
    "type(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TextBlob object has a number of attribures that have processed the text. The simplest are lists of words and sentences. Here we pull just the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is obviously a better approach than the one we took when we just split a string on spaces -- a technique that didn't handle punctuation like commas and periods well. OK that's a good trick but there are better ones! For example, TextBlob's language processing let's it estimate which words are part of noun phrases. \n",
    "\n",
    "There are various techniques for doing this and none of them are perfect. To be fair, using a headline means using a text fragment and not a sentence. The language processing tools are usually trained on full sentences of text. Still, it's not bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun phrases are obtained by extracting information from a \"tagged\" version of the text. Here the tags represent parts of speech. You can see [a complete list of the tags here.](https://cs.nyu.edu/grishman/jet/guide/PennPOS.html) The parts of speech are stored as a list of word-tag pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tb.tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .tags attribute is a list. (See the square brackets?) The list elements are a new data type called a \"tuple\" which is like a list, for our purposes. So you can take, say the first element of the tags list and look at the first and second elements of the tuple (the word and its estimates part of speech)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.tags[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.tags[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I'm not wild about it, TextBlob also provides an estimate of the sentiment of the statement. That is, is the text expressing a positive or negative sentiment. I'll leave you to consult the Parrish blog post or the TextBlob documentation of this lovely feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing. There are various methods to \"parse\" text -- different algorithms for tagging words in a sentence, for extracting noun phrases and for estimating sentiment. You can replace the default when you call TextBlob. The documentation describes other noun phrase extractors. Here's how you would use the ConllExtractor, based on a data set compiled for the Conference on Computational Natural Language Learning (CoNLL-2000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.np_extractors import ConllExtractor\n",
    "extractor = ConllExtractor()\n",
    "\n",
    "tb = TextBlob(headline,np_extractor=extractor)\n",
    "tb.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
