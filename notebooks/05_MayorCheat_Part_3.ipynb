{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/computationaljournalism/columbia2020/master/images/cl.jpeg\" width=800>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**From last time - looking at the tweets**\n",
    "\n",
    "Using the Premium API we pulled tweets per day containing the hashtag `#MayorCheat`. Most of the action took place on the 4th of February, so let's start there. We have a file of just over 100k tweets from that day, each one containing the term #MayorCheat. We have put them up on [Dropbox](https://www.dropbox.com/s/x1alcns5mxga60c/mayorcheat_202002040000_202002050000.json?dl=0). Download the file and put it in the same folder as this notebook. \n",
    "\n",
    "Recall that each line in the file is a JSON string representing a tweet from February 4 containing the hashtag `#MayorCheat`. Let's read the data into a list, one tweet-string per entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day1 = open(\"mayorcheat_202002040000_202002050000.json\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(day1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to make some of the information in the file easier to work with by flattening it into a DataFrame. We created a CSV [located here](https://github.com/computationaljournalism/columbia2020/raw/master/data/mc/mayorcheat_all_04.csv.gz) so you don't have to do the steps below. But you should see what we did and think about why it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need to execute this one\n",
    "\n",
    "#build = []\n",
    "\n",
    "#for tweet_str in day1:\n",
    "\n",
    "#    tweet = loads(tweet_str)\n",
    "\n",
    "#    who_rt = \"\"\n",
    "#    text_rt = \"\"\n",
    "    \n",
    "#    if \"retweeted_status\" in tweet:\n",
    "#        who_rt = tweet[\"retweeted_status\"][\"user\"][\"screen_name\"]\n",
    "#        text_rt = tweet[\"retweeted_status\"][\"text\"]\n",
    "        \n",
    "#    newdata = {\"created_at\":tweet[\"created_at\"],\n",
    "#               \"screen_name\":tweet[\"user\"][\"screen_name\"],\n",
    "#               \"text\":tweet[\"text\"],\n",
    "#               \"followers_count\":tweet[\"user\"][\"followers_count\"],\n",
    "#               \"friends_count\":tweet[\"user\"][\"friends_count\"],\n",
    "#               \"retweeted_user\":who_rt,\n",
    "#               \"retweeted_text\":text_rt}\n",
    "    \n",
    "#    build.append(newdata)\n",
    "               \n",
    "#from pandas import DataFrame\n",
    "\n",
    "# build a dataframe and output a CSV\n",
    "#day1_df = DataFrame(build)\n",
    "#day1_df.to_csv(\"mayorcheat_all_04.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "day1 = read_csv(\"mayorcheat_all_04.csv\")\n",
    "day1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, we didn't do this last time but the `.str` object in a DataFrame column lets us do string-like things to entire columns. Here we test to see which tweet text contains `\"Cernovich\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import set_option\n",
    "set_option('max_colwidth', -1)\n",
    "\n",
    "day1[day1[\"text\"].str.contains(\"Cernovich\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at those entries in our DataFrme that represent retweets and pull them into a separate structure. We use the option `.copy(deep=True)` to create an entirely independent copy of our data frame. Whatever changes we make to this, stay with this copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets = day1[~day1[\"retweeted_user\"].isnull()].copy(deep=True)\n",
    "retweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets[\"retweeted_user\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look to see if there are well-worn retweet patterns. We can just paste together two columns with a space in between to get us a string that holds the person retweeting, a space, and then the person being retweeted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets[\"fromto\"] = retweets[\"screen_name\"]+\" \"+retweets[\"retweeted_user\"]\n",
    "retweets[\"fromto\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.express import histogram\n",
    "\n",
    "fig = histogram(retweets, x=\"eastern\",nbins=200)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this data frame of retweets is one way to represent the activity taking place around the conversation. So we can think of users as nodes in a network with an arrow running from one to the other if the first node was retweeted by the second node. So to do this, let's break time up into chunks. Here's \"Hour 1\" or the first hour into the life of the hashtag. Simple subsetting gives us all the `retweets` rows that occurred before 3am EST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour1 = retweets[retweets[\"eastern\"]< \"2020-02-04 03:00:00\"].copy(deep=True)\n",
    "hour1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then looked at [graphcommons.com](http://graphcommons.com), a site for making shared network graphs. I love this tool. So we need a CSV with columns FromType, FromName, Edge, ToType, ToName, Weight. We'll do that below, making three new columns (FromType and ToType and Edge type), and then rename two columns to FromName and ToName."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FromType, FromName, Edge, ToType, ToName\n",
    "\n",
    "hour1[\"FromType\"] = \"User\"\n",
    "hour1[\"ToType\"] = \"User\"\n",
    "hour1[\"Edge\"] = \"Retweeted by\"\n",
    "hour1 = hour1.rename(columns={\"retweeted_user\":\"FromName\",\"screen_name\":\"ToName\"})\n",
    "hour1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next bit of code is slightly advanced, but we'll narrate it and come back later. It basically take repeated retweet events (someone retweets the same person 10 times) and replaces the 10 entries with just one having a Weight of 10.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour1_weights = hour1[[\"FromType\",\"FromName\",\"Edge\",\"ToType\",\"ToName\"]].groupby([\"FromType\",\"FromName\",\"Edge\",\"ToType\",\"ToName\"]).size().reset_index().rename(columns={0:'Weight'})\n",
    "hour1_weights.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then output the file to `hour1.csv` that we can read into graph commons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour1_weights.to_csv(\"hour1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done this for seven hours into the event which takes us basically up to the first peak. Now, you can either download the data in CSV format and upload it to graph commons, or use the graphs I have created linked here. The code for each CSV is in a separate cell -- although wouldn't a loop be better?!?!\n",
    "\n",
    "\n",
    "* [Hour 1](https://graphcommons.com/graphs/57a029ac-3eea-4ff9-ada4-1d4b3b0fd171)\n",
    "* [Hour 2](https://graphcommons.com/graphs/9cb5b4a8-06f2-4ba1-88d4-a44f5b6d9351)\n",
    "* [Hour 3](https://graphcommons.com/graphs/f93f4c4c-23f5-411a-8bd1-52f9428b7499)\n",
    "* [Hour 4](https://graphcommons.com/graphs/58fd0297-a720-4567-b54f-f3a066e4d80c)\n",
    "* [Hour 5](https://graphcommons.com/graphs/79fbf890-030b-4440-bd50-091a8e929680)\n",
    "* [Hour 6](https://graphcommons.com/graphs/cac35b27-8289-49f9-9178-6698ad681e27)\n",
    "* [Hour 7](https://graphcommons.com/graphs/c028797d-f692-4394-b9c1-7dff0dc9cb31)\n",
    "\n",
    "We'll talk about what do do with Graph Commons next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour2 = retweets[(retweets[\"eastern\"]< \"2020-02-04 04:00:00\") &\n",
    "                 (retweets[\"eastern\"]> \"2020-02-04 03:00:00\")].copy(deep=True)\n",
    "\n",
    "hour2[\"FromType\"] = \"User\"\n",
    "hour2[\"ToType\"] = \"User\"\n",
    "hour2[\"Edge\"] = \"Retweeted by\"\n",
    "\n",
    "hour2 = hour2.rename(columns={\"retweeted_user\":\"FromName\",\"screen_name\":\"ToName\"})\n",
    "hour2_weights = hour2[[\"FromType\",\"FromName\",\"Edge\",\"ToType\",\"ToName\"]].groupby([\"FromType\",\"FromName\",\"Edge\",\"ToType\",\"ToName\"]).size().reset_index().rename(columns={0:'Weight'})\n",
    "hour2_weights.to_csv(\"hour2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour3 = retweets[(retweets[\"eastern\"]< \"2020-02-04 05:00:00\") &\n",
    "                 (retweets[\"eastern\"]> \"2020-02-04 04:00:00\")].copy(deep=True)\n",
    "\n",
    "hour3[\"FromType\"] = \"User\"\n",
    "hour3[\"ToType\"] = \"User\"\n",
    "hour3[\"Edge\"] = \"Retweeted by\"\n",
    "\n",
    "hour3 = hour3.rename(columns={\"retweeted_user\":\"FromName\",\"screen_name\":\"ToName\"})\n",
    "hour3_weights = hour3[[\"FromType\",\"FromName\",\"Edge\",\"ToType\",\"ToName\"]].groupby([\"FromType\",\"FromName\",\"Edge\",\"ToType\",\"ToName\"]).size().reset_index().rename(columns={0:'Weight'})\n",
    "hour3_weights.to_csv(\"hour3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour4 = retweets[(retweets[\"eastern\"]< \"2020-02-04 06:00:00\") &\n",
    "                 (retweets[\"eastern\"]> \"2020-02-04 05:00:00\")].copy(deep=True)\n",
    "\n",
    "hour4[\"FromType\"] = \"User\"\n",
    "hour4[\"ToType\"] = \"User\"\n",
    "hour4[\"Edge\"] = \"Retweeted by\"\n",
    "\n",
    "hour4 = hour4.rename(columns={\"retweeted_user\":\"FromName\",\"screen_name\":\"ToName\"})\n",
    "hour4_weights = hour4[[\"FromType\",\"FromName\",\"Edge\",\"ToType\",\"ToName\"]].groupby([\"FromType\",\"FromName\",\"Edge\",\"ToType\",\"ToName\"]).size().reset_index().rename(columns={0:'Weight'})\n",
    "hour4_weights.to_csv(\"hour4.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour5 = retweets[(retweets[\"eastern\"]< \"2020-02-04 07:00:00\") &\n",
    "                 (retweets[\"eastern\"]> \"2020-02-04 06:00:00\")].copy(deep=True)\n",
    "\n",
    "hour5[\"FromType\"] = \"User\"\n",
    "hour5[\"ToType\"] = \"User\"\n",
    "hour5[\"Edge\"] = \"Retweeted by\"\n",
    "\n",
    "hour5 = hour5.rename(columns={\"retweeted_user\":\"FromName\",\"screen_name\":\"ToName\"})\n",
    "hour5_weights = hour5[[\"FromType\",\"FromName\",\"Edge\",\"ToType\",\"ToName\"]].groupby([\"FromType\",\"FromName\",\"Edge\",\"ToType\",\"ToName\"]).size().reset_index().rename(columns={0:'Weight'})\n",
    "hour5_weights.to_csv(\"hour5.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour6 = retweets[(retweets[\"eastern\"]< \"2020-02-04 07:00:00\") &\n",
    "                 (retweets[\"eastern\"]> \"2020-02-04 06:00:00\")].copy(deep=True)\n",
    "\n",
    "hour6[\"FromType\"] = \"User\"\n",
    "hour6[\"ToType\"] = \"User\"\n",
    "hour6[\"Edge\"] = \"Retweeted by\"\n",
    "\n",
    "hour6 = hour6.rename(columns={\"retweeted_user\":\"FromName\",\"screen_name\":\"ToName\"})\n",
    "hour6_weights = hour6[[\"FromType\",\"FromName\",\"Edge\",\"ToType\",\"ToName\"]].groupby([\"FromType\",\"FromName\",\"Edge\",\"ToType\",\"ToName\"]).size().reset_index().rename(columns={0:'Weight'})\n",
    "hour6_weights.to_csv(\"hour6.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour7 = retweets[(retweets[\"eastern\"]< \"2020-02-04 07:00:00\") &\n",
    "                 (retweets[\"eastern\"]> \"2020-02-04 06:00:00\")].copy(deep=True)\n",
    "\n",
    "hour7[\"FromType\"] = \"User\"\n",
    "hour7[\"ToType\"] = \"User\"\n",
    "hour7[\"Edge\"] = \"Retweeted by\"\n",
    "\n",
    "hour7 = hour7.rename(columns={\"retweeted_user\":\"FromName\",\"screen_name\":\"ToName\"})\n",
    "hour7_weights = hour7[[\"FromType\",\"FromName\",\"Edge\",\"ToType\",\"ToName\"]].groupby([\"FromType\",\"FromName\",\"Edge\",\"ToType\",\"ToName\"]).size().reset_index().rename(columns={0:'Weight'})\n",
    "hour7_weights.to_csv(\"hour7.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is here to help you skim through the users as you identify them, going back to the original data. You can also skim through the hourly files you created your network graphs from..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day1[day1[\"screen_name\"]==\"jonrocks69\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
