{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro to Web Scraping\n",
    "--------------------------\n",
    "\n",
    "While open data portals, APIs and other publication mechanisms provide easy ways to get to information we need for our analysys and reporting, there are plenty other valuable data sources for us to take advantage of: web pages (HTML), PDF files, email dumps, etc. Automating the extraction of useful information from web pages is known as **\"web scraping.\"** A terrible name aside, web scraping is very powerful and it's something you'll want to master. Today, we'll close our session talking about some of the basics of web scraping in Python.\n",
    "\n",
    "![Web Scraping](https://blog.hartleybrody.com/wp-content/uploads/2012/12/scraper-tool.jpg)\n",
    "\n",
    "There are many ways to scrape information from the web, but we're going to use Python, [requests](http://docs.python-requests.org/en/master/) and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COVID Tracking State Data\n",
    "---------------------\n",
    "\n",
    "The COVID Tracking project is trying to assemble state data on testing. They collect data from each state and scrape the basic testing statistics [Their descriptions of each state's data are here](https://covidtracking.com/data/). Let's focus on California.\n",
    "\n",
    "[Here is the page](https://www.cdph.ca.gov/Programs/CID/DCDC/Pages/Immunization/ncov2019.aspx). Have a look and tell me what kind of information is available. Compare it to data from a couple other states. \n",
    "\n",
    "Now, the first part of web scraping is making HTTP requests to pull the pages we need into Python. We will use the [requests](http://docs.python-requests.org/en/master/) library again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the request to get the Trump Lies HTML\n",
    "from requests import get\n",
    "\n",
    "url = 'https://www.cdph.ca.gov/Programs/CID/DCDC/Pages/Immunization/ncov2019.aspx'\n",
    "\n",
    "headers = {\n",
    "    'From': '<put your email here>',\n",
    "}\n",
    "response = get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we include a \"header\" field (represented as a dictionary). The header passes information to the web server that might change the way it returns content. In later exercises, we might need to specify the header \"User-Agent\" which tells the server what kind of  browser the requeste is being made from -- some servers don't like handing pages out to bots. \n",
    "\n",
    "For now, we are using the \"From\" header to announce ourselves. I like to tell a source that I'm taking data. If you want to know more about headers, have a look [here](https://en.wikipedia.org/wiki/List_of_HTTP_header_fields)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what we have. remember that response.text will give us a string value of the page HTML\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is kind of a mess. The whole web page has been read in as a string. Thankfully, one of the great things about Python is a package called [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/), designed by [Leonard Richardson](http://www.crummy.com/self/). It is truly a thing of beauty. BeautifulSoup is a parser for HTML (and XML) that creates an object that lets you interact with the components of a web page. You can search for tags, extract attributes from the tags and pull the content contained in a tag. [The documentation is pretty simple too.](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) The latest version of BeautifulSoup is 4.6.0 and the package is called bs4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An example**\n",
    "\n",
    "We will come back to the COVID data with BeautifulSoup but let's start with a simple example first. Here is some very simple HTML that I'd like to run through BeautifulSoup:\n",
    "\n",
    "```html\n",
    "<html>\n",
    "\n",
    "    <head>\n",
    "        <title>My Technology News Site</title>\n",
    "    </head>\n",
    "\n",
    "    <body>\n",
    "        <div>\n",
    "            <p class=\"title\"><strong>Steve Jobs introduces the public beta of Mac OS X</strong></p>\n",
    "            <div class=\"description\">Sept 13, 2000 - Steve Jobs <a href=\"https://www.apple.com/pr/library/2000/09/13Apple-Releases-Mac-OS-X-Public-Beta.html\" target=\"_blank\">introduces</a> the public beta of Mac OS X for US$29.95.</div>\n",
    "            <div class=\"author\">Author: Michael Young</div>\n",
    "        </div>\n",
    "    </body>\n",
    "\n",
    "</html>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "our_html = '''\n",
    "<html>\n",
    "\n",
    "    <head>\n",
    "        <title>My Technology News Site\n",
    "    </head>\n",
    "\n",
    "    <body>\n",
    "        <div>\n",
    "            <p class=\"title\"><strong>Steve Jobs introduces the public beta of Mac OS X</strong></p>\n",
    "            <div class=\"description\">Sept 13, 2000 - Steve Jobs <a href=\"https://www.apple.com/pr/library/2000/09/13Apple-Releases-Mac-OS-X-Public-Beta.html\" target=\"_blank\">introduces</a> the public beta of Mac OS X for US$29.95.</div>\n",
    "            <div class=\"author\">Author: Michael Young</div>\n",
    "        </div>\n",
    "    </body>\n",
    "\n",
    "</html>\n",
    "'''\n",
    "\n",
    "# BeautifulSoup takes two arguments: a string (hopefully with HTML in it) and the parser we'd like to use\n",
    "soup = BeautifulSoup(our_html, 'html.parser')\n",
    "\n",
    "# print out a pretty version of the BeautifulSoup object\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a super-quick review of [HTML](https://en.wikipedia.org/wiki/HTML):\n",
    "\n",
    "Hypertext Markup Language is the language used for creating web pages. HTML uses `tags` which help label as well as structure the data in the document. Web browsers use the tags to help render the web page but does not display the tags. \n",
    "\n",
    "`Tags` normall come in pairs and have an opening tag `<p>` and a closing tag `</p>`:\n",
    "```html\n",
    "<p>this is a paragraph tag</p>\n",
    "```\n",
    "\n",
    "Other tags like the image tag `<img>` don't have a closing tag:\n",
    "```html\n",
    "<img src=\"http://somesite.com/images/logo.jpg\" />\n",
    "```\n",
    "\n",
    "The other important thing about HTML tags is that they can contain one or more `attributes`. Like in the `<img>` tag above, the `src` attribute is used specify the URL of the image. A tag with multiple attributes could look like this:\n",
    "```html\n",
    "<p attribute_1=\"value1\" attribute_2=\"value2\">\n",
    "Our content goes here\n",
    "</p>\n",
    "```\n",
    "\n",
    "HTML documents typically have nested tags (think of a tree!) that looks like this:\n",
    "```html\n",
    "<html>\n",
    "  <head>\n",
    "    <title>My First Website!</title>\n",
    "  </head>\n",
    "\n",
    "    <body>\n",
    "        <p>My mom would be proud of this.</p>\n",
    "    </body>  \n",
    "</html>\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to BeautifulSoup...**\n",
    "\n",
    "When we run our HTML document through BeautifulSoup, we get a python object that allows us to traverse, query and manipulate the HTML document.\n",
    "\n",
    "Here are a few ways to inspect our simple HTML document that we loaded above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <title> tag\n",
    "print(soup.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the <title> tag\n",
    "print(soup.title.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string value in the <title> tag\n",
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about if we want to find the first <div> tag?\n",
    "\n",
    "soup.div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the string value for the first <p> tag within the first <div>\n",
    "\n",
    "soup.div.p.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the value of the \"class\" attribute of the first <div> under the first <div> (!?!)\n",
    "\n",
    "soup.div.div['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is how we'd find the the <a> tag\n",
    "\n",
    "soup.div.div.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For You To Try\n",
    "\n",
    "# how would you find the url in the description?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `find` and `find_all` to search through the HTML to find certains tags and tag/attribute combinations. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all <p> tags\n",
    "for p in soup.find_all('p'):\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all <div class=\"author>...</div> tags\n",
    "for author_div in soup.find_all('div', attrs={'class': 'author'}):\n",
    "    print(author_div.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COVID Tracking**\n",
    "\n",
    "Coming back to California's COVID page, how can we use BeautifulSoup to parse our the different categories like ages and genders? And then how might we store the data for comparison over time?\n",
    "\n",
    "Let's start by pulling the HTML for the page and parsing it with Beautiful Soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.cdph.ca.gov/Programs/CID/DCDC/Pages/Immunization/ncov2019.aspx'\n",
    "\n",
    "headers = {\n",
    "    'From': '<put your email here>'\n",
    "}\n",
    "\n",
    "# http request\n",
    "response = get(url, headers=headers)\n",
    "\n",
    "# run the HTML through BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# print out a pretty version of the BeautifulSoup object\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a mess! Let's open up the [link](https://www.cdph.ca.gov/Programs/CID/DCDC/Pages/Immunization/ncov2019.aspx) in Chrome and view the HTML there. The `View Page Source` option  allows us to peek behind the scenes of any web page. Another great resource are the [Chrome Developer Tools](https://developer.chrome.com/devtools) which give you an even greater look at how a page is constructed.\n",
    "\n",
    "Do we see any patterns in the California COVID HTML?\n",
    "\n",
    "```html\n",
    "<h2>COVID-19 By the Numbers<br/></h2><p>As of March 27, 2020, 2 p.m. Pacific Daylight Time, there are a total of 4,643&#160;positive cases and 101&#160;deaths in California (including one non-California resident).&#160;</p><ul style=\"list-style-type: disc;\"><li>923: Community-acquired cases</li><li>3,720: Cases acquired through person-to-person transmission, travel (including cruise ship passengers), repatriation, or under investigation.</li><ul style=\"list-style-type: circle;\"><li>This includes 73&#160;health care workers</li></ul></ul><p><strong>Ages of all confirmed positive cases:</strong></p><ul style=\"list-style-type: disc;\"><li>Age 0-17: 54&#160;cases</li><li>Age 18-49: 2,368&#160;cases</li><li>Age 50-64: 1,184&#160;cases<br/></li><li>Age 65 and older: 1,016&#160;cases</li><li>Unknown: 21&#160;cases</li></ul><p><strong>Gender of all confirmed positive cases:</strong></p><ul style=\"list-style-type: disc;\"><li>Female: 2,057&#160;cases</li><li>Male: 2,536&#160;cases</li><li>Unknown: 50&#160;cases<br/><br/></li></ul>\n",
    "```\n",
    "\n",
    "It seems as though most of the data are presented as unordered lists of various styles. That's a `<ul>` tag with some kind of `style=` attribute. We notice, for example that the age breakdown has style `\"list-style-type: disc;\"`, but so do several other categorizations. \n",
    "\n",
    "Let's pull all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can find each category between the <ul style=\"list-style-type: disc;\"> and </ul> tags\n",
    "\n",
    "for category in soup.find_all('ul', attrs={\"style\": \"list-style-type: disc;\"}): \n",
    "    print(category.prettify())\n",
    "    print(\"---\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see some categories and then the same style is used for a list of things you can do to protect yourself from the virus. Now, the age breakdown is the second set of `<ul>` tags. The `find_all()` method produces a list-like object meaning we can get at the second one simply by using  square brackets with the index 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('ul', attrs={\"style\": \"list-style-type: disc;\"})[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or pretty\n",
    "\n",
    "print(soup.find_all('ul', attrs={\"style\": \"list-style-type: disc;\"})[1].prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now, let's store this in a variable and within this tag, we will search for all the `<li>` tags and extract the text. Each tag has a method called `get_text()` that does just what we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how would we find the age breakdown?\n",
    "\n",
    "ages = soup.find_all('ul', attrs={\"style\": \"list-style-type: disc;\"})[1]\n",
    "\n",
    "for age_range in ages.find_all('li'):\n",
    "    \n",
    "    # from the list item, we extract the \n",
    "    print(age_range.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now store this in a number of ways. Right now, each row is a string. How could we clean up these strings and store them so we can track them over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your suggestions here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Searching around in a document**\n",
    "\n",
    "We can use BeautifulSoup to search for tags that contain certain text as well. As you might guess, the search involves specifying patterns using regular expressions. To find the header `<h2>` that has the phrase \"COVID-19 By the Numbers\", we can simply use this string as its own regular expression (we are matching literals). (We use the command `compile()` to create a pattern object that can be applied across the text in each tag, returning those that match.)\n",
    "\n",
    "Because we know there is only one expression \"COVID-19 By the Numbers\" on the page, we can use `find()` to find the first one as opposed to `find_all()`. Below we also look at the tag that comes next after the `<h2>` that we want and then the tag after that, using subsequent calls to `next_element()`. \n",
    "\n",
    "Eventually we end up with the string that gives the date of the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import compile\n",
    "\n",
    "header = soup.find(text=compile(r\"COVID-19 By the Numbers\"))\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next tag after the one with the text we wanted\n",
    "print(header.next_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the tag after that\n",
    "print(header.next_element.next_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we use `get_text()` to pull the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = header.next_element.next_element.get_text()\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we have to extract the date and maybe the total case counts. Regular expressions to the rescue again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x1f3c6; **Challenge round!** &#x1f3c6;\n",
    "\n",
    "Pick one or two of these tasks and use your skills with web scraping to answer the question. In each case, there is a URL and a data question attached to it. These come mainly from an excellent list compiled by Dan Nguyen at Stanford.\n",
    "\n",
    ">Site: [https://analytics.usa.gov/](https://analytics.usa.gov/)<br>\n",
    "Task: Number of people visiting US Government web sites now<br><br>\n",
    "Site: [http://www.state.gov/r/pa/ode/socialmedia/](http://www.state.gov/r/pa/ode/socialmedia/)<br>\n",
    "Task: The number of Pinterest accounts maintained by U.S. State Department embassies and missions<br><br>\n",
    "Site: [https://petitions.whitehouse.gov/](https://petitions.whitehouse.gov/)<br>\n",
    "Task: Number of petitions that have reached their goal<br><br>\n",
    "Site: [https://www.faa.gov/air_traffic/flight_info/aeronav/aero_data/](https://www.faa.gov/air_traffic/flight_info/aeronav/aero_data/)<br>\n",
    "Task: Number of airports with existing construction related activity<br><br>\n",
    "Site: [https://www.osha.gov/pls/imis/establishment.html](https://www.osha.gov/pls/imis/establishment.html)<br>\n",
    "Number of OSHA enforcement inspections involving Wal-Mart in California since 2014<br><br>\n",
    "Site: [https://www.tdcj.state.tx.us/death_row/dr_scheduled_executions.html](https://www.tdcj.state.tx.us/death_row/dr_scheduled_executions.html)<br>\n",
    "Task: Number of days until Texas's next scheduled execution <br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
